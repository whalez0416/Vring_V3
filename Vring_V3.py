#ì½œë°±í•¸ë“¤ëŸ¬ ì˜¤ë¥˜ í•´ê²° í•„ìš” 
#ë©”ì‹œì§€ ë¡œê·¸ ì‚¬ë¼ì§€ì§€ ì•ŠëŠ” ë¬¸ì œ í•´ê²° í•„ìš”

import os ## ìš´ì˜ì²´ì œì™€ ì—°ë™í•˜ê¸° ìœ„í•œ ëª¨ë“ˆ
import json ## ë©”ì‹œì§€ë¥¼ json í˜•ì‹ìœ¼ë¡œ ì €ì¥í•˜ê¸° ìœ„í•œ ëª¨ë“ˆ
import streamlit as st #ìŠ¤íŠ¸ë¦¼ë¦¿ ì—°ê²°
from langchain.prompts import ChatPromptTemplate #í”„ë¡¬í”„íŠ¸ë¥¼ í†µí•œ llm instruction
from langchain.document_loaders import UnstructuredFileLoader # pdf, txt, word ë“± ë‹¤ì–‘í•œ íŒŒì¼ ë“±ë¡ ê°€ëŠ¥í•˜ë„ë¡ document_loaders ì„¤ì¹˜
from langchain.embeddings import OpenAIEmbeddings #ì˜¤í”ˆ ai ì„ë² ë”©
from langchain.schema.runnable import RunnableLambda, RunnablePassthrough ###
from langchain.text_splitter import RecursiveCharacterTextSplitter #ì¬ê·€ì  ìºë¦­í„°í…ìŠ¤íŠ¸ìŠ¤í”Œë¦¿í„°, ë°˜ì , ì˜¨ì  ë“±ì„ ê¸°ì¤€ìœ¼ë¡œ ëŠìŒ
from langchain.vectorstores.faiss import FAISS #ë°±í„° ìŠ¤í† ì–´
from langchain.callbacks.base import BaseCallbackHandler ### ì½œë°±í•¸ë“¤ëŸ¬ ëª¨ë“ˆ
from langchain.chat_models import ChatOpenAI #llm
from langchain.retrievers.multi_query import MultiQueryRetriever #ë¦¬íŠ¸ë¦¬ë²„
from google.cloud import storage #ìºì‰¬ë¥¼ í´ë¼ìš°ë“œì—ë„ ì €ì¥ë˜ë„ë¡ êµ¬ê¸€ í´ë¼ìš°ë“œ ì—°ê²° 
from google.oauth2 import service_account #êµ¬ê¸€ í´ë¼ìš°ë“œ ì„œë¹„ìŠ¤ ì¸ì¦ì²˜ë¦¬
import pickle # ì €ì¥ë˜ëŠ” ë°ì´í„°ë¥¼ ë³´ë‹¤ ë¹ ë¥´ê²Œ ë¶ˆëŸ¬ì˜¤ê³ , ë³µì¡í•œ ë°ì´í„°ë¥¼ íš¨ìœ¨ì ìœ¼ë¡œ ì €ì¥í•˜ê¸° ìœ„í•œ ëª¨ë“ˆ
import tempfile #ë°ì´í„°ë¥¼ ë¡œì»¬ì— ì„ì‹œ ì €ì¥í•˜ê¸° ìœ„í•œ ëª¨ë“ˆ

# Streamlit secrets ì— toml íŒŒì¼ë¡œ êµ¬ê¸€ í‚¤ê°€ ì—…ë°ì´íŠ¸ê°€ ì•ˆë˜ë‹ˆ êµ¬ê¸€ í´ë¼ìš°ë“œ í‚¤ë¥¼ secretsì„œ ì½ì„ìˆ˜ ìˆë„ë¡ í•˜ëŠ” í‚¤ 
service_account_info = st.secrets["gcp_service_account"]
credentials = service_account.Credentials.from_service_account_info(service_account_info)
storage_client = storage.Client(credentials=credentials)


# Streamlit í˜ì´ì§€ ì„¤ì •
st.set_page_config(page_title="Vring_V3", page_icon="ğŸ“ƒ")

# Google Cloud Storageì— íŒŒì¼ ì—…ë¡œë“œí•˜ëŠ” í•¨ìˆ˜ ë²„ì¼“ ì´ë¦„, ë¡œì»¬ì— ìˆëŠ” ì—…ë¡œë“œí•  íŒŒì¼ ì´ë¦„, gcsì— ì €ì¥í•  blob ì„ ë°›ìŒ  
def upload_to_gcs(bucket_name, source_file_name, destination_blob_name):
    try: 
        #ë²„ì¼“ì„ í˜¸ì¶œ
        bucket = storage_client.bucket(bucket_name)
        #ë¸”ëì€ ëŒ€ê·œëª¨ ì´ì§„ ë°ì´í„° ì €ì¥í•˜ëŠ” ê²ƒ ì•ì„œ ì •ì˜í•œ ë²„ì¼“ì„ ì´ì§„ ë°ì´í„°ë¡œ ì €ì¥
        blob = bucket.blob(destination_blob_name)
        #ì €ì¥í•œ ì´ì§„ ë°ì´í„°ë¥¼ ì•ì„œ ë§Œë“  íŒŒì¼ ë„¤ì„ê³¼ ë™ì¼í•˜ê²Œ ì—…ë¡œë“œ
        blob.upload_from_filename(source_file_name)
        #íŒŒì¼ì´ì´ ì—…ë¡œë“œ ë˜ë©´ í•´ë‹¹ ë¬¸êµ¬ë¥¼ ì¶œë ¥í•¨ "{ë¡œì»¬íŒŒì¼ì´ë¦„}ì´ {ë²„ì¼“ì´ë¦„}/{ì´ì§„íŒŒì¼ì´ë¦„}ì— ì €ì¥ë˜ì—ˆë‹¤"
        print(f"{source_file_name} has been uploaded to {bucket_name}/{destination_blob_name}")
    #ì˜ˆì™¸ê°€ ë°œìƒí–ˆì„ë•ŒëŠ” ì—…ë¡œë“œì— ì‹¤íŒ¨í–ˆë‹¤ëŠ” ë¬¸êµ¬ë¥¼ ì¶œë ¥
    except Exception as e:
        print(f"Failed to upload {source_file_name} to GCS: {e}")
        
        
        
# ë©”ì‹œì§€ ì €ì¥ í•¨ìˆ˜: ì¹´ì¹´ì˜¤í†¡ ì²˜ëŸ¼ ëŒ€í™”ê°€ ê³„ì† ëˆ„ì ë˜ë©° ì €ì¥ë  ìˆ˜ ìˆë„ë¡ í•˜ê¸° ìœ„í•œ í•¨ìˆ˜
def save_message(message, role): 
    #ìŠ¤íŠ¸ë¦¼ë¦¿ ì„¸ì…˜ ìŠ¤í…Œì´íŠ¸(íŠ¹ì • ìƒíƒœë¥¼ ì €ì¥í•  ìˆ˜ ìˆëŠ” ìƒíƒœ) ì•ˆì— ì•„ë¬´ëŸ° ë©”ì‹œì§€ê°€ ì—†ë‹¤ë©´ ë¹ˆ ë¦¬ìŠ¤íŠ¸ë¥¼ ì¶œë ¥
    if "messages" not in st.session_state:
        #ì—¬ê¸°ì„œ ëŒ€ê´„í˜¸ëŠ” ë”•ì…”ë„ˆë¦¬ì™€ ê°™ì´ ì´ë¯¸ ì €ì¥ëœ messagesë¥¼ ì¶œë ¥í•˜ëŠ” ê²ƒì´ê³  ì´ë¯¸ ì €ì¥ëœ ë©”ì‹œì§€ê°€ ì—†ë‹¤ë©´ ë¹ˆ ë¦¬ìŠ¤íŠ¸ë¥¼ ì¶œë ¥í•˜ë¼ëŠ” ëœ» 
        st.session_state["messages"] = [] 
    #ì•ì„œ ì¶œë ¥ëœ ë¹ˆë¦¬ìŠ¤íŠ¸ì— ë©”ì‹œì§€ì™€ roleì„ ì§€ì†ì ìœ¼ë¡œ ì„¸ì…˜ìŠ¤í…Œì´íŠ¸ì— ì¶”ê°€ë˜ë„ë¡ ì„¤ì •
    st.session_state["messages"].append({"message": message, "role": role}) 
    #ì´ ëŒ€í™” ê¸°ë¡ì„ json íŒŒì¼ë¡œ ì €ì¥í•˜ë„ë¡ ì„¤ì • messages.jsonì€ íŒŒì¼ ì´ë¦„ "w"ëŠ” ì“°ê¸° ëª¨ë“œ(ì“°ê¸° ëª¨ë“œëŠ” ê¸°ì¡´ íŒŒì¼ì´ ìˆìœ¼ë©´ ë‚´ìš©ì„ ë®ì–´ì“°ê³ , íŒŒì¼ì´ ì—†ìœ¼ë©´ ìƒˆë¡œ ë§Œë“¬.) ì´ê±¸ as fë¡œ ë°›ìŒ
    with open("messages.json", "w") as f:  
        #dumpëŠ” ë¦¬ìŠ¤íŠ¸ì¸ íŒŒì´ì¬ ê°ì²´ë¥¼ json íŒŒì¼ë¡œ ë³€í™˜ í•˜ëŠ”ë° ì—¬ê¸°ì„œëŠ” ì„¸ì…˜ìŠ¤í…Œì´íŠ¸ì— ì €ì¥ëœ messagesë¥¼ ë³€í™˜í•˜ê³  ì•ì— ì •ì˜í•œf ì—ë‹¤ê°€ ê¸°ë¡
        json.dump(st.session_state["messages"], f) 
    #ê·¸ë¦¬ê³  ê·¸ê±¸ ì•ì„œ ì •ì˜í•œ êµ¬ê¸€ í´ë¼ìš°ë“œ ì—…ë¡œë“œ í•¨ìˆ˜ë¡œ ì—…ë¡œë“œ í•¨ 
    upload_to_gcs('goldgpt_v2', 'messages.json', 'messages.json')

#ì½œë°± í•¸ë“¤ëŸ¬: í”„ë¡œê·¸ë¨ì´ ì‘ë™ë˜ëŠ” ìƒí™©ì„ ì„¸ë¶„í™”í•˜ê³  ê·¸ ê³¼ì •ë§ˆë‹¤ íŠ¹ì • ëª…ë ¹ì„ ë‚´ë¦´ ìˆ˜ ìˆëŠ” ëª¨ë“ˆ
class ChatCallbackHandler(BaseCallbackHandler):
    #ì´ í´ë˜ìŠ¤ ì•„ë˜ í¬í•¨ë˜ëŠ” ëª¨ë“  ë©”ì†Œë“œì— ì˜í–¥ì„ ë¼ì¹˜ëŠ” ì†ì„± ê°’ìœ¼ë¡œ ëª¨ë“  ë©”ì‹œì§€ëŠ” ë¹ˆ ë¬¸ìì—´ë¡œ ì¶œë ¥í•˜ë„ë¡ ì„¤ì •
    message = ""
    #llmì´ ì‹œì‘í• ë•Œ ë©”ì‹œì§€ ë°•ìŠ¤ì— st.emptyë¡œ ë¹ˆ ìš”ì†Œë¥¼ ë§Œë“¬
    def on_llm_start(self, *args, **kwargs):
        self.message_box = st.empty()
        self.message = ""  # ë©”ì‹œì§€ ì´ˆê¸°í™”
    #llmì´ ì¢…ë£Œë ë•Œ save_message í•¨ìˆ˜ë¡œ ë©”ì‹œì§€ë¥¼ json í˜•ì‹ìœ¼ë¡œ ì €ì¥ã„´
    def on_llm_end(self, *args, **kwargs):
        save_message(self.message, "ai")
    #í† í°ì´ ì¶”ê°€ë ë•Œë§ˆë‹¤ ë§ˆí¬ë‹¤ìš´ í˜•ì‹ìœ¼ë¡œ ë¹ˆ ë¬¸ìì—´ì¸ ë©”ì‹œì§€ì— ì±„ì›Œì§€ë„ë¡ ì„¤ì •í•˜ì—¬ ìŠ¤íŠ¸ë¦¬ë° í˜•íƒœë¡œ ë³´ì—¬ì§€ë„ë¡ í•¨ 
    def on_llm_new_token(self, token, *args, **kwargs):
        self.message += token
        self.message_box.markdown(self.message)


# LLM ì„¤ì •
llm = ChatOpenAI(
    temperature=0.1,
    streaming=True,
    callbacks=[ChatCallbackHandler(),],
)



# Google Cloud Storageì—ì„œ íŒŒì¼ ë‹¤ìš´ë¡œë“œ ì´ê±¸ ì™œí•˜ëŠ”ê±°ì§€ ?
def download_from_gcs(bucket_name, source_blob_name, destination_file_name):
    try:
        bucket = storage_client.bucket(bucket_name)
        blob = bucket.blob(source_blob_name)
        blob.download_to_filename(destination_file_name)
        print(f"{source_blob_name} has been downloaded to {destination_file_name}")
    except Exception as e:
        print(f"Failed to download {source_blob_name} from GCS: {e}")

# Google Cloud Storageì—ì„œ íŒŒì¼ ë‹¤ìš´ë¡œë“œí•˜ì—¬ ì„ì‹œ íŒŒì¼ë¡œ ë¡œë“œ(ìœ ì €ê°€ ì—…ë¡œë“œ í•œ íŒŒì¼ì€ ì„ë² ë”©í›„ ì—…ë¡œë“œê°€ ë˜ë©°, ì—…ë¡œë“œ ëœ íŒŒì¼ì€ ë¡œì»¬ì—ë„ ìë™ì €ì¥ë˜ì–´ì•¼í•¨)
def load_from_gcs(bucket_name, source_blob_name):
    try:
        bucket = storage_client.bucket(bucket_name)
        blob = bucket.blob(source_blob_name)
        temp_local_filename = tempfile.mkstemp()
        blob.download_to_filename(temp_local_filename)
        with open(temp_local_filename, "rb") as f:
            data = pickle.load(f)
        os.remove(temp_local_filename)
        return data
    except Exception as e:
        print(f"Failed to load {source_blob_name} from GCS: {e}")
        return None

# Google Cloud Storageì— íŒŒì¼ ì—…ë¡œë“œí•˜ì—¬ ì €ì¥
def save_to_gcs(bucket_name, destination_blob_name, data):
    try:
        bucket = storage_client.bucket(bucket_name)
        blob = bucket.blob(destination_blob_name)
        _, temp_local_filename = tempfile.mkstemp()
        with open(temp_local_filename, "wb") as f:
            pickle.dump(data, f)
        blob.upload_from_filename(temp_local_filename)
        os.remove(temp_local_filename)
        print(f"{destination_blob_name} has been saved to {bucket_name}")
    except Exception as e:
        print(f"Failed to save {destination_blob_name} to GCS: {e}")

# íŒŒì¼ ì„ë² ë”© ë° ìºì‹± í•¨ìˆ˜
def embed_file(file):
    temp_dir = tempfile.mkdtemp()
    file_path = os.path.join(temp_dir, file.name)
    file_content = file.read()
    
    with open(file_path, "wb") as f:
        f.write(file_content)
    
    bucket_name = 'goldgpt_v2'  # GCS ë²„í‚· ì´ë¦„
    upload_to_gcs(bucket_name, file_path, file.name)

    cache_file_path = f"embeddings/{file.name}.pkl"
    
    multiquery_retriever = None  # ì´ˆê¸°í™”

    # ìºì‹œ íŒŒì¼ ì¡´ì¬ ì—¬ë¶€ í™•ì¸
    vectorstore = load_from_gcs(bucket_name, cache_file_path)
    if vectorstore is None:
        try:
            st.write("Creating new embeddings...")
            loader = UnstructuredFileLoader(file_path)
            splitter = RecursiveCharacterTextSplitter(
                chunk_size=600,
                chunk_overlap=100,
            )
            docs = loader.load_and_split(text_splitter=splitter)
            st.write(f"Loaded and split {len(docs)} documents.")
            embeddings = OpenAIEmbeddings()
            vectorstore = FAISS.from_documents(docs, embeddings)
            st.write("Created FAISS vectorstore.")
            
            # FAISS ì¸ë±ìŠ¤ ì €ì¥
            save_to_gcs(bucket_name, cache_file_path, vectorstore)
            
            multiquery_retriever = MultiQueryRetriever.from_llm(
                retriever=vectorstore.as_retriever(), llm=llm
            )
            st.write("Created MultiQueryRetriever.")
        except Exception as e:
            st.error(f"Failed to create embeddings: {e}")
            print(f"Failed to create embeddings: {e}")
    else:
        try:
            st.write("Loaded cached embeddings...")
            multiquery_retriever = MultiQueryRetriever.from_llm(
                retriever=vectorstore.as_retriever(), llm=llm
            )
            st.write("Loaded MultiQueryRetriever from cache.")
        except Exception as e:
            st.error(f"Failed to load cached embeddings: {e}")
            print(f"Failed to load cached embeddings: {e}")

    if multiquery_retriever is None:
        raise ValueError("Failed to create or load embeddings.")
    
    return multiquery_retriever


# í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿
prompt = ChatPromptTemplate.from_messages([
    ("system", """
        you are the head of professional consultation at the urology department. Our goal is to attract patients to our hospital by responding to all inquiries with the utmost kindness and warmth. you provide answers based only on the given context. If you are uncertain about a query, you kindly and warmly encourage the inquirer to contact the hospital directly for more information..
        Additionally, to make patients feel more at ease, I use emoticons like the following:
        Example 1: Hello~ Good morning!
        Example 2: Thank you for your question ^^ The answer is as follows!
        If I have already ì•ˆë…•í•˜ì„¸ìš”, I will not repeat ì•ˆë…•í•˜ì„¸ìš”.
        Context: {context}
    """),
    ("human", "{question}"),
])

# Streamlit UI ì„¤ì •
st.title("Vring_V3")
st.markdown("""
ê¶ê¸ˆí•œ ë‚´ìš©ì„ ì±—ë´‡ì—ê²Œ ë¬¼ì–´ë³´ì‹œë©´ ë‹µë³€ë“œë¦¬ê² ìŠµë‹ˆë‹¤.
""")





# ë©”ì‹œì§€ ë³´ë‚´ê¸°
def send_message(message, role, save=True):
    with st.chat_message(role):
        st.markdown(message)
    if save:
        save_message(message, role)
        
# íŒŒì¼ ì—…ë¡œë“œ ì²˜ë¦¬
with st.sidebar:
    file = st.file_uploader(
        "Upload a .txt .pdf or .docx file",
        type=["pdf", "txt", "docx"],
    )
    
    
# ë¬¸ì„œ í¬ë§·íŒ…
def format_docs(docs):
    return "\n\n".join(document.page_content for document in docs)

# íŒŒì¼ì´ ì—…ë¡œë“œë˜ê±°ë‚˜ ì‚­ì œë  ë•Œ ì„¸ì…˜ ìƒíƒœ ì´ˆê¸°í™”
if file:
    retriever = embed_file(file)
    send_message("I'm ready! Ask away!", "ai", save=False)
    message = st.chat_input("Ask anything about your file...")
    if message:
        send_message(message, "human")
        chain = (
            {
                "context": retriever | RunnableLambda(format_docs),
                "question": RunnablePassthrough(),
            }
            | prompt
            | llm
        )
        with st.chat_message("ai"):
            chain.invoke(message)


else:
    st.session_state["messages"] = [] 